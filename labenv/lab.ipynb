{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8f57f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysandboxonto.observation mysandboxonto.owl#observation\n"
     ]
    }
   ],
   "source": [
    "# This script imports a Ontology and copies 2 classes and saves it in a new ontology\n",
    "\n",
    "from owlready2 import get_ontology, Thing\n",
    "import types\n",
    "\n",
    "onto_path = \"mysandboxonto.owl\"\n",
    "onto_to_import = [\"../data/ontologies/local_import/ppeo.owl\", \"../data/ontologies/local_import/sosa.owl\"] \n",
    "\n",
    "try:\n",
    "    onto.destroy()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "onto = get_ontology(onto_path)\n",
    "\n",
    "try:\n",
    "    onto.imported_ontologies.clear()\n",
    "except:\n",
    "    pass\n",
    "onto.imported_ontologies.extend([get_ontology(o).load() for o in onto_to_import])\n",
    "\n",
    "\n",
    "def create_class(cls):\n",
    "    with onto:\n",
    "        NewType = types.new_class(cls.name, (cls, ))\n",
    "      \n",
    "        # NewType.iri = cls.iri\n",
    "   \n",
    "\n",
    "def checkexistingClass(cname):\n",
    "    for c in onto.classes():\n",
    "        if c.name == cname:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# for c in onto.imported_ontologies[0].classes():\n",
    "#     print(c.name)\n",
    "\n",
    "# for prop in onto.imported_ontologies[0].properties():\n",
    "#     print(prop)\n",
    "\n",
    "# lets take tuple as input\n",
    "inp = (\"data_file\", \"hasObservation\", \"observation\")\n",
    "\n",
    "\n",
    "\n",
    "i = 2\n",
    "if not checkexistingClass(inp[i]):\n",
    "    create_class(onto.imported_ontologies[0][inp[i]])\n",
    "\n",
    "for c in onto.classes():\n",
    "    print(c, c.iri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "90484486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://purl.org/ppeo/PPEO.owl#GPS_location\n",
      "http://purl.org/ppeo/PPEO.owl#biological_material\n",
      "http://purl.org/ppeo/PPEO.owl#country\n",
      "http://purl.org/ppeo/PPEO.owl#data_file\n",
      "http://purl.org/ppeo/PPEO.owl#environment\n",
      "http://purl.org/ppeo/PPEO.owl#environment_parameter\n",
      "http://purl.org/ppeo/PPEO.owl#event\n",
      "http://purl.org/ppeo/PPEO.owl#factor\n",
      "http://purl.org/ppeo/PPEO.owl#factor_value\n",
      "http://purl.org/ppeo/PPEO.owl#growth_facility\n",
      "http://purl.org/ppeo/PPEO.owl#institution\n",
      "http://purl.org/ppeo/PPEO.owl#investigation\n",
      "http://purl.org/ppeo/PPEO.owl#location\n",
      "http://purl.org/ppeo/PPEO.owl#material_source\n",
      "http://purl.org/ppeo/PPEO.owl#method\n",
      "http://purl.org/ppeo/PPEO.owl#named_location\n",
      "http://purl.org/ppeo/PPEO.owl#observation\n",
      "http://purl.org/ppeo/PPEO.owl#observation_level\n",
      "http://purl.org/ppeo/PPEO.owl#observation_level_hierarchy\n",
      "http://purl.org/ppeo/PPEO.owl#observation_unit\n",
      "http://purl.org/ppeo/PPEO.owl#observed_variable\n",
      "http://purl.org/ppeo/PPEO.owl#person\n",
      "http://purl.org/ppeo/PPEO.owl#role\n",
      "http://purl.org/ppeo/PPEO.owl#sample\n",
      "http://purl.org/ppeo/PPEO.owl#scale\n",
      "http://purl.org/ppeo/PPEO.owl#spatial_coordinate\n",
      "http://purl.org/ppeo/PPEO.owl#spatial_distribution\n",
      "http://purl.org/ppeo/PPEO.owl#spatial_distribution_type\n",
      "http://purl.org/ppeo/PPEO.owl#experimental_design\n",
      "http://purl.org/ppeo/PPEO.owl#study\n",
      "http://purl.org/ppeo/PPEO.owl#trait\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "ppeo = \"../data/ontologies/local_import/ppeo.owl\"\n",
    "\n",
    "parser = etree.XMLParser(\n",
    "        load_dtd=True,\n",
    "        resolve_entities=True,\n",
    "        ns_clean=True,\n",
    "        recover=True,\n",
    "    )\n",
    "tree = etree.parse(ppeo, parser)\n",
    "\n",
    "root = tree.getroot()\n",
    "\n",
    "for child in root:\n",
    "    if child.tag == \"{http://www.w3.org/2002/07/owl#}Class\":\n",
    "        print(child.get(\"{http://www.w3.org/1999/02/22-rdf-syntax-ns#}about\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6c474e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  Ontology Merger\n",
      "=================================================================\n",
      "\n",
      "  [ppeo]  IRI : http://purl.org/ppeo/PPEO.owl\n",
      "          File: ../data/ontologies/local_import/ppeo.owl\n",
      "          Entities indexed: 124\n",
      "\n",
      "  [sosa]  IRI : http://www.w3.org/ns/sosa/\n",
      "          File: ../data/ontologies/local_import/sosa.owl\n",
      "          Entities indexed: 53\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "  Processing triplets …\n",
      "\n",
      "  Triplet from [ppeo] : (data_file, hasObservation, observation)\n",
      "    ✓  Subject  (Class): http://purl.org/ppeo/PPEO.owl#data_file\n",
      "    ✓  Predicate (Property): http://purl.org/ppeo/PPEO.owl#hasObservation\n",
      "    ✓  Object   (Class): http://purl.org/ppeo/PPEO.owl#observation\n",
      "\n",
      "  Triplet from [sosa] : (Observation, madeBySensor, Sensor)\n",
      "    ✓  Subject  (Class): http://www.w3.org/ns/sosa/Observation\n",
      "    ✓  Predicate (Property): http://www.w3.org/ns/sosa/madeBySensor\n",
      "    ✓  Object   (Class): http://www.w3.org/ns/sosa/Sensor\n",
      "\n",
      "  Generating stubs for referenced entities …\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasDescription\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasDigitalLocation\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasVersion\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasValue\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasVariable\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#observed_variable\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#observation_unit\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#sample\n",
      "    ↳  stub: http://purl.org/ppeo/PPEO.owl#hasObservedSubject\n",
      "    ↳  stub: http://www.w3.org/ns/sosa/madeObservation\n",
      "\n",
      "=================================================================\n",
      "  ✓  Merged ontology written to:\n",
      "     /home/gryvity/Desktop/workstation/lab/MIAPPExSOSA/labenv/merged_ontology.owl\n",
      "  ✓  Fully extracted entities : 6\n",
      "  ✓  Stub declarations added  : 10\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Ontology Merger\n",
    "===============\n",
    "Extracts selected classes and properties from source OWL ontologies\n",
    "based on (subject, predicate, object) triplets, and assembles them\n",
    "into a new OWL ontology while:\n",
    "  - Preserving original IRIs verbatim\n",
    "  - Copying all OWL axioms and restrictions in full (blank-node closures)\n",
    "  - Adding stub declarations for referenced-but-not-selected entities\n",
    "  - Annotating provenance (source ontology IRI + local name) on every entity\n",
    "\n",
    "Usage:  python3 ontology_merger.py\n",
    "Output: merged_ontology.owl  (RDF/XML)\n",
    "\"\"\"\n",
    "\n",
    "from lxml import etree\n",
    "import copy, re, os\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "SOURCE_FILES = {\n",
    "    \"ppeo\": \"../data/ontologies/local_import/ppeo.owl\",\n",
    "    \"sosa\": \"../data/ontologies/local_import/sosa.owl\",\n",
    "}\n",
    "\n",
    "OUTPUT_FILE = \"/home/gryvity/Desktop/workstation/lab/MIAPPExSOSA/labenv/merged_ontology.owl\"\n",
    "\n",
    "NEW_ONTOLOGY_IRI = \"http://example.org/merged-ontology/1.0\"\n",
    "\n",
    "# Each triplet: (subject_localname, predicate_localname, object_localname, source_key)\n",
    "TRIPLETS = [\n",
    "    (\"data_file\",    \"hasObservation\", \"observation\", \"ppeo\"),\n",
    "    (\"Observation\",  \"madeBySensor\",   \"Sensor\",      \"sosa\"),\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 2. NAMESPACE CONSTANTS  (Clark-notation helpers)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "NS = {\n",
    "    \"rdf\":   \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "    \"rdfs\":  \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "    \"owl\":   \"http://www.w3.org/2002/07/owl#\",\n",
    "    \"xsd\":   \"http://www.w3.org/2001/XMLSchema#\",\n",
    "    \"dc\":    \"http://purl.org/dc/elements/1.1/\",\n",
    "    \"skos\":  \"http://www.w3.org/2004/02/skos/core#\",\n",
    "    \"schema\":\"http://schema.org/\",\n",
    "    \"prov\":  \"http://www.w3.org/ns/prov#\",\n",
    "    \"merge\": \"http://example.org/merged-ontology/\",\n",
    "}\n",
    "\n",
    "def ck(ns_key, local):\n",
    "    \"\"\"Clark notation: {namespace}localname\"\"\"\n",
    "    return f\"{{{NS[ns_key]}}}{local}\"\n",
    "\n",
    "RDF_ABOUT    = ck(\"rdf\", \"about\")\n",
    "RDF_RESOURCE = ck(\"rdf\", \"resource\")\n",
    "RDF_TYPE     = ck(\"rdf\", \"type\")\n",
    "RDFS_LABEL   = ck(\"rdfs\", \"label\")\n",
    "RDFS_COMMENT = ck(\"rdfs\", \"comment\")\n",
    "RDFS_DOMAIN  = ck(\"rdfs\", \"domain\")\n",
    "RDFS_RANGE   = ck(\"rdfs\", \"range\")\n",
    "OWL_ONTOLOGY = ck(\"owl\", \"Ontology\")\n",
    "OWL_CLASS    = ck(\"owl\", \"Class\")\n",
    "OWL_OBJPROP  = ck(\"owl\", \"ObjectProperty\")\n",
    "OWL_DATPROP  = ck(\"owl\", \"DatatypeProperty\")\n",
    "OWL_ANNPROP  = ck(\"owl\", \"AnnotationProperty\")\n",
    "OWL_IMPORTS  = ck(\"owl\", \"imports\")\n",
    "RDFS_SEEALSO = ck(\"rdfs\", \"seeAlso\")\n",
    "\n",
    "SKIP_NAMESPACES = {\n",
    "    NS[\"rdf\"], NS[\"rdfs\"], NS[\"owl\"], NS[\"xsd\"],\n",
    "    NS[\"skos\"], NS[\"schema\"],\n",
    "    \"http://purl.org/dc/\", \"http://purl.org/dc/terms/\",\n",
    "    \"http://purl.org/dc/elements/\",\n",
    "    \"http://xmlns.com/foaf/\",\n",
    "    \"http://purl.org/vocab/vann/\",\n",
    "    \"http://purl.org/vocommons/\",\n",
    "}\n",
    "\n",
    "def is_builtin_iri(iri: str) -> bool:\n",
    "    return any(iri.startswith(ns) for ns in SKIP_NAMESPACES)\n",
    "\n",
    "def local_name(iri: str) -> str:\n",
    "    return iri.split(\"#\")[-1].split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. PARSING SOURCE ONTOLOGIES\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def parse_owl_file(path: str):\n",
    "    \"\"\"\n",
    "    Parse an OWL/RDF-XML file, resolving DOCTYPE entity references (e.g. &sosa;).\n",
    "    Returns the lxml ElementTree root.\n",
    "    \"\"\"\n",
    "    parser = etree.XMLParser(\n",
    "        load_dtd=True,\n",
    "        resolve_entities=True,\n",
    "        ns_clean=True,\n",
    "        recover=True,\n",
    "    )\n",
    "    tree = etree.parse(path, parser)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_ontology_iri(root):\n",
    "    \"\"\"\n",
    "    Extract the ontology IRI from the document.\n",
    "    Handles both the standard <owl:Ontology rdf:about=\"...\"> form (ppeo)\n",
    "    and the <owl:NamedIndividual><rdf:type rdf:resource=\"...Ontology\"/> form (sosa).\n",
    "    Falls back to xml:base on the root element.\n",
    "    \"\"\"\n",
    "    OWL_NAMED_IND = ck(\"owl\", \"NamedIndividual\")\n",
    "    # 1. Standard form\n",
    "    for child in root:\n",
    "        if child.tag == OWL_ONTOLOGY:\n",
    "            iri = child.get(RDF_ABOUT) or child.get(ck(\"rdf\", \"ID\"))\n",
    "            if iri:\n",
    "                return iri\n",
    "    # 2. NamedIndividual with rdf:type owl:Ontology (sosa style)\n",
    "    for child in root:\n",
    "        if child.tag == OWL_NAMED_IND:\n",
    "            iri = child.get(RDF_ABOUT)\n",
    "            if iri:\n",
    "                for sub in child:\n",
    "                    if sub.tag == RDF_TYPE:\n",
    "                        res = sub.get(RDF_RESOURCE, \"\")\n",
    "                        if res.endswith(\"Ontology\"):\n",
    "                            return iri\n",
    "    # 3. Fall back to xml:base on the root <rdf:RDF> element\n",
    "    XML_NS = \"http://www.w3.org/XML/1998/namespace\"\n",
    "    base = root.get(f\"{{{XML_NS}}}base\")\n",
    "    if base:\n",
    "        return base\n",
    "    return None\n",
    "\n",
    "\n",
    "def index_entities(root):\n",
    "    \"\"\"\n",
    "    Build a dict:  { local_name -> [(element, full_iri, element_type_tag)] }\n",
    "    Covers owl:Class, owl:ObjectProperty, owl:DatatypeProperty, owl:AnnotationProperty.\n",
    "    \"\"\"\n",
    "    index = {}\n",
    "    TARGET_TAGS = {OWL_CLASS, OWL_OBJPROP, OWL_DATPROP, OWL_ANNPROP}\n",
    "    for elem in root:\n",
    "        if elem.tag not in TARGET_TAGS:\n",
    "            continue\n",
    "        iri = elem.get(RDF_ABOUT)\n",
    "        if not iri:\n",
    "            continue\n",
    "        ln = local_name(iri)\n",
    "        index.setdefault(ln, []).append((elem, iri, elem.tag))\n",
    "    return index\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 4. COLLECTING ALL IRIs REFERENCED INSIDE AN ELEMENT (for stub generation)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def collect_referenced_iris(elem):\n",
    "    \"\"\"\n",
    "    Walk the element tree and gather every rdf:resource / rdf:about value\n",
    "    that isn't a blank-node or built-in namespace IRI.\n",
    "    \"\"\"\n",
    "    iris = set()\n",
    "    for node in elem.iter():\n",
    "        for attr in (RDF_RESOURCE, RDF_ABOUT):\n",
    "            val = node.get(attr)\n",
    "            if val and not val.startswith(\"_:\") and not is_builtin_iri(val):\n",
    "                iris.add(val)\n",
    "    return iris\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. ADDING PROVENANCE ANNOTATION TO A COPIED ELEMENT\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "MERGE_SOURCED_FROM = f\"{{{NS['merge']}}}sourcedFrom\"\n",
    "MERGE_SOURCE_ONT   = f\"{{{NS['merge']}}}sourceOntology\"\n",
    "\n",
    "def annotate_provenance(elem, source_key: str, source_ont_iri: str):\n",
    "    \"\"\"\n",
    "    Append two rdfs:comment-style provenance annotations to the copied element,\n",
    "    so the original IRI context is always traceable in the OWL file.\n",
    "    \"\"\"\n",
    "    ann1 = etree.SubElement(elem, MERGE_SOURCED_FROM)\n",
    "    ann1.text = source_ont_iri\n",
    "    ann1.set(ck(\"rdf\", \"datatype\"), NS[\"xsd\"] + \"anyURI\")\n",
    "\n",
    "    ann2 = etree.SubElement(elem, MERGE_SOURCE_ONT)\n",
    "    ann2.text = source_key\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 6. BUILDING THE MERGED ONTOLOGY ROOT\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def build_merged_root() -> etree._Element:\n",
    "    \"\"\"\n",
    "    Create the <rdf:RDF> root element with all namespace declarations\n",
    "    and the <owl:Ontology> header.\n",
    "    \"\"\"\n",
    "    ns_map = {\n",
    "        \"rdf\":    NS[\"rdf\"],\n",
    "        \"rdfs\":   NS[\"rdfs\"],\n",
    "        \"owl\":    NS[\"owl\"],\n",
    "        \"xsd\":    NS[\"xsd\"],\n",
    "        \"dc\":     NS[\"dc\"],\n",
    "        \"skos\":   NS[\"skos\"],\n",
    "        \"schema\": NS[\"schema\"],\n",
    "        \"prov\":   NS[\"prov\"],\n",
    "        \"merge\":  NS[\"merge\"],\n",
    "    }\n",
    "    root = etree.Element(ck(\"rdf\", \"RDF\"), nsmap=ns_map)\n",
    "    # xml:base uses a special W3C namespace\n",
    "    root.set(\"{http://www.w3.org/XML/1998/namespace}base\", NEW_ONTOLOGY_IRI)\n",
    "\n",
    "    # ── owl:Ontology header ──\n",
    "    ont = etree.SubElement(root, OWL_ONTOLOGY)\n",
    "    ont.set(RDF_ABOUT, NEW_ONTOLOGY_IRI)\n",
    "\n",
    "    title = etree.SubElement(ont, ck(\"dc\", \"title\"))\n",
    "    title.text = \"Merged Ontology\"\n",
    "    title.set(\"{http://www.w3.org/XML/1998/namespace}lang\", \"en\")\n",
    "\n",
    "    desc = etree.SubElement(ont, ck(\"dc\", \"description\"))\n",
    "    desc.text = (\n",
    "        \"A new ontology that selectively imports classes and properties \"\n",
    "        \"from PPEO and SOSA, preserving original IRIs and all OWL axioms.\"\n",
    "    )\n",
    "    desc.set(\"{http://www.w3.org/XML/1998/namespace}lang\", \"en\")\n",
    "\n",
    "    return root\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 7. STUB GENERATOR  (ensures referenced entities are at least declared)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def make_stub(iri, source_index, source_ont_iri, source_key):\n",
    "    \"\"\"\n",
    "    Return a minimal owl:Class / owl:ObjectProperty element for an IRI\n",
    "    that appears in a restriction but wasn't requested as a full entity.\n",
    "    Tries to pull label/comment from the source index; falls back to a\n",
    "    bare declaration.\n",
    "    \"\"\"\n",
    "    ln = local_name(iri)\n",
    "    hits = source_index.get(ln, [])\n",
    "\n",
    "    # If found in source, copy just the direct metadata (no sub-restrictions)\n",
    "    for (src_elem, src_iri, src_tag) in hits:\n",
    "        if src_iri == iri:\n",
    "            stub = etree.Element(src_tag)\n",
    "            stub.set(RDF_ABOUT, iri)\n",
    "            # Copy only label, comment, rdfs:isDefinedBy (no subClassOf / restrictions)\n",
    "            for child in src_elem:\n",
    "                if child.tag in (\n",
    "                    RDFS_LABEL, RDFS_COMMENT,\n",
    "                    ck(\"rdfs\", \"isDefinedBy\"),\n",
    "                    ck(\"skos\", \"definition\"),\n",
    "                ):\n",
    "                    stub.append(copy.deepcopy(child))\n",
    "            annotate_provenance(stub, source_key, source_ont_iri)\n",
    "            return stub\n",
    "\n",
    "    # Fallback: bare declaration (we know it's a class from context)\n",
    "    stub = etree.Element(OWL_CLASS)\n",
    "    stub.set(RDF_ABOUT, iri)\n",
    "    lbl = etree.SubElement(stub, RDFS_LABEL)\n",
    "    lbl.text = ln\n",
    "    lbl.set(\"{http://www.w3.org/XML/1998/namespace}lang\", \"en\")\n",
    "    return stub\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 8. MAIN  –  assemble the merged ontology\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 65)\n",
    "    print(\"  Ontology Merger\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "    # ── Parse source files ──\n",
    "    sources = {}\n",
    "    for key, path in SOURCE_FILES.items():\n",
    "        root = parse_owl_file(path)\n",
    "        ont_iri = get_ontology_iri(root) or f\"(unknown – {path})\"\n",
    "        idx = index_entities(root)\n",
    "        sources[key] = {\"root\": root, \"iri\": ont_iri, \"index\": idx}\n",
    "        print(f\"\\n  [{key}]  IRI : {ont_iri}\")\n",
    "        print(f\"          File: {path}\")\n",
    "        print(f\"          Entities indexed: {sum(len(v) for v in idx.values())}\")\n",
    "\n",
    "    # ── Build merged root ──\n",
    "    merged_root = build_merged_root()\n",
    "\n",
    "    # Add rdfs:seeAlso pointers to source ontologies in the header\n",
    "    ont_header = merged_root[0]  # the owl:Ontology element\n",
    "    for key, info in sources.items():\n",
    "        sa = etree.SubElement(ont_header, RDFS_SEEALSO)\n",
    "        sa.set(RDF_RESOURCE, info[\"iri\"])\n",
    "\n",
    "    # ── Process each triplet — PASS 1: collect full extractions ──\n",
    "    print(\"\\n\" + \"-\" * 65)\n",
    "    print(\"  Processing triplets …\")\n",
    "\n",
    "    full_entities   = {}    # iri -> (elem, src_key)   — full axiom copies\n",
    "    ref_iri_to_src  = {}    # iri -> (src_key, src_iri) — for stub generation\n",
    "\n",
    "    for (subj_ln, pred_ln, obj_ln, src_key) in TRIPLETS:\n",
    "        info  = sources[src_key]\n",
    "        idx   = info[\"index\"]\n",
    "        o_iri = info[\"iri\"]\n",
    "        print(f\"\\n  Triplet from [{src_key}] : ({subj_ln}, {pred_ln}, {obj_ln})\")\n",
    "\n",
    "        for role, ln, expected_tags in [\n",
    "            (\"Subject  (Class)\",     subj_ln, {OWL_CLASS}),\n",
    "            (\"Predicate (Property)\", pred_ln, {OWL_OBJPROP, OWL_DATPROP}),\n",
    "            (\"Object   (Class)\",     obj_ln,  {OWL_CLASS}),\n",
    "        ]:\n",
    "            hits = idx.get(ln, [])\n",
    "            matched = [(e, iri, tag) for (e, iri, tag) in hits if tag in expected_tags]\n",
    "            if not matched:\n",
    "                matched = [(e, iri, tag) for (e, iri, tag) in hits]\n",
    "            if not matched:\n",
    "                print(f\"    ⚠  {role}: '{ln}' not found in {src_key}!\")\n",
    "                continue\n",
    "\n",
    "            src_elem, full_iri, tag = matched[0]\n",
    "            print(f\"    ✓  {role}: {full_iri}\")\n",
    "\n",
    "            if full_iri not in full_entities:\n",
    "                elem_copy = copy.deepcopy(src_elem)\n",
    "                annotate_provenance(elem_copy, src_key, o_iri)\n",
    "                full_entities[full_iri] = (elem_copy, src_key)\n",
    "\n",
    "                # Collect referenced IRIs for stub generation\n",
    "                for ref_iri in collect_referenced_iris(src_elem):\n",
    "                    if ref_iri == full_iri:\n",
    "                        continue\n",
    "                    if ref_iri not in ref_iri_to_src:\n",
    "                        ref_iri_to_src[ref_iri] = (src_key, o_iri)\n",
    "\n",
    "    # ── PASS 2: generate stubs for referenced IRIs not fully included ──\n",
    "    print(\"\\n  Generating stubs for referenced entities …\")\n",
    "    stubs = {}  # iri -> elem\n",
    "\n",
    "    for ref_iri, (src_key, src_ont_iri) in ref_iri_to_src.items():\n",
    "        if ref_iri in full_entities:\n",
    "            continue                         # already fully extracted\n",
    "        if is_builtin_iri(ref_iri):\n",
    "            continue                         # skip owl/rdf/xsd builtins\n",
    "        # Skip bare ontology namespace IRIs (end with \"/\" and are the source ont IRI)\n",
    "        if ref_iri.endswith(\"/\") or ref_iri in {info[\"iri\"] for info in sources.values()}:\n",
    "            continue\n",
    "\n",
    "        ref_ln = local_name(ref_iri)\n",
    "        found_in = None\n",
    "        for try_key in [src_key] + [k for k in sources if k != src_key]:\n",
    "            hits2 = sources[try_key][\"index\"].get(ref_ln, [])\n",
    "            for (e2, i2, t2) in hits2:\n",
    "                if i2 == ref_iri:\n",
    "                    found_in = (e2, i2, t2, try_key, sources[try_key][\"iri\"])\n",
    "                    break\n",
    "            if found_in:\n",
    "                break\n",
    "\n",
    "        if found_in:\n",
    "            e2, i2, t2, fk, fo = found_in\n",
    "            stub = make_stub(ref_iri, sources[fk][\"index\"], fo, fk)\n",
    "        else:\n",
    "            stub = make_stub(ref_iri, {}, \"\", src_key)\n",
    "\n",
    "        if stub is not None:\n",
    "            stubs[ref_iri] = stub\n",
    "            print(f\"    ↳  stub: {ref_iri}\")\n",
    "\n",
    "    # ── Append full entities (sorted by source, then local name) ──\n",
    "    for iri, (elem, src_key) in sorted(full_entities.items(),\n",
    "                                        key=lambda kv: (kv[1][1], local_name(kv[0]))):\n",
    "        merged_root.append(elem)\n",
    "\n",
    "    # ── Append stubs (sorted) ──\n",
    "    for iri, stub in sorted(stubs.items(), key=lambda kv: local_name(kv[0])):\n",
    "        merged_root.append(stub)\n",
    "\n",
    "    # ── Serialize ──\n",
    "    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "    tree = etree.ElementTree(merged_root)\n",
    "    tree.write(\n",
    "        OUTPUT_FILE,\n",
    "        pretty_print=True,\n",
    "        xml_declaration=True,\n",
    "        encoding=\"UTF-8\",\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(f\"  ✓  Merged ontology written to:\")\n",
    "    print(f\"     {OUTPUT_FILE}\")\n",
    "    print(f\"  ✓  Fully extracted entities : {len(full_entities)}\")\n",
    "    print(f\"  ✓  Stub declarations added  : {len(stubs)}\")\n",
    "    print(\"=\" * 65)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6297f7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Investigation hasAssociatedPublication\n",
      "Sample deriveFrom\n",
      "Person hasAffiliation\n",
      "ObservationUnit hasBiologicalMaterial\n",
      "ObservationUnit hasEvent\n",
      "ObservationUnit hasObservationLevel\n",
      "Study hasBiologicalMaterial\n",
      "Study hasContactInstitution\n",
      "Study hasEnvironment\n",
      "Study hasEvent\n",
      "Study hasFactor\n",
      "Study hasGrowthFacility\n",
      "Study hasObservationLevelHierarchy\n",
      "DataFile hasObservation\n",
      "ObservedVariable hasMethod\n",
      "Environment hasEnvironmentParameter\n",
      "Factor hasFactorValue\n",
      "Factor hasModality\n",
      "FactorValue isModalityOf\n",
      "BiologicalMaterial hasCountryOfOrigin\n"
     ]
    }
   ],
   "source": [
    "# READ YAML\n",
    "\n",
    "import yaml\n",
    "\n",
    "config = \"/home/gryvity/Desktop/workstation/lab/MIAPPExSOSA/labenv/miappexsosa_config.yaml\"\n",
    "\n",
    "with open(config) as file:\n",
    "    content = yaml.safe_load(file)\n",
    "\n",
    "for s, p, o in content[\"imports\"][\"ppeo\"][\"triplets\"]:\n",
    "    print(s, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "excel_to_jsonld.py\n",
    "==================\n",
    "Converts a structured Excel checklist + an OWL ontology into:\n",
    "  1.  A JSON-LD document whose @graph mirrors the ontology's class/property\n",
    "      structure (nested graph, hasPart / partOf resolved as object links).\n",
    "  2.  A Cypher import script ready for Neo4j.\n",
    "\n",
    "Usage\n",
    "-----\n",
    "    python excel_to_jsonld.py <excel_file> <owl_file>\n",
    "                              [--companion <companion_ttl>]\n",
    "                              [--output-jsonld <path>]\n",
    "                              [--output-cypher <path>]\n",
    "\n",
    "Design\n",
    "------\n",
    "Each worksheet represents one OWL class.  The first row contains property\n",
    "names (column headers).  A sheet may be:\n",
    "  - \"record-per-row\"  →  each row is one instance (Study, Person …)\n",
    "  - \"transposed\"      →  row-1 = property names, row-2 = single values\n",
    "                         (detected when column A header is \"property\" and\n",
    "                          column A value is \"value\").\n",
    "\n",
    "Special column names\n",
    "  hasPart / partOf    →  ObjectProperty; value is a semicolon-separated\n",
    "                         list of IDs referencing rows in another sheet.\n",
    "  Person:hasXxx       →  a property from an external vocabulary (schema.org /\n",
    "                         foaf); mapped by the companion ontology.\n",
    "\n",
    "All class and property URIs are looked up in the merged OWL graph\n",
    "(base ontology + optional companion).  Unknown column headers are kept as\n",
    "hydata: terms so no data is silently dropped.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import openpyxl\n",
    "from rdflib import Graph, Namespace, OWL, RDF, RDFS, URIRef, Literal\n",
    "from rdflib.namespace import XSD\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Namespaces\n",
    "# ---------------------------------------------------------------------------\n",
    "PPEO    = Namespace(\"http://purl.org/ppeo/PPEO.owl#\")\n",
    "HYDATA  = Namespace(\"http://purl.org/hydata/ontology#\")\n",
    "SCHEMA  = Namespace(\"https://schema.org/\")\n",
    "FOAF    = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "\n",
    "BASE_PREFIXES = {\n",
    "    \"ppeo\":    str(PPEO),\n",
    "    \"hydata\":  str(HYDATA),\n",
    "    \"schema\":  str(SCHEMA),\n",
    "    \"foaf\":    str(FOAF),\n",
    "    \"xsd\":     str(XSD),\n",
    "    \"rdf\":     \"http://www.w3.org/1999/02/22-rdf-syntax-ns#\",\n",
    "    \"rdfs\":    \"http://www.w3.org/2000/01/rdf-schema#\",\n",
    "    \"owl\":     \"http://www.w3.org/2002/07/owl#\",\n",
    "}\n",
    "\n",
    "# Properties treated as object links (→ reference other named nodes)\n",
    "OBJECT_LINK_PROPS = {\"hasPart\", \"partOf\", \"derivesFrom\",\n",
    "                     \"hasContactInstitution\", \"hasAffiliation\",\n",
    "                     \"hasLocation\", \"hasCountry\"}\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _slug(text: str) -> str:\n",
    "    \"\"\"Create a simple URI-safe slug from free text.\"\"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-]\", \"_\", str(text).strip())\n",
    "\n",
    "\n",
    "def _local(uri: str | URIRef) -> str:\n",
    "    \"\"\"Return the local name of a URI (after # or last /).\"\"\"\n",
    "    s = str(uri)\n",
    "    return s.split(\"#\")[-1] if \"#\" in s else s.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "\n",
    "def _parse_date(val: Any) -> str | None:\n",
    "    \"\"\"Return an ISO date string or None.\"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "    if isinstance(val, (datetime, date)):\n",
    "        return val.strftime(\"%Y-%m-%d\")\n",
    "    try:\n",
    "        # Excel may store dates as integers (days since 1899-12-30)\n",
    "        ref = datetime(1899, 12, 30)\n",
    "        d = ref + __import__(\"datetime\").timedelta(days=int(val))\n",
    "        return d.strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return str(val).strip() or None\n",
    "\n",
    "\n",
    "def _split_ids(raw: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split a semicolon- (or comma-) separated list of IDs.\n",
    "    Handles messy entries like '2018_SEL;, 2019_WST; 2019_NRS'\n",
    "    \"\"\"\n",
    "    if not raw:\n",
    "        return []\n",
    "    parts = re.split(r\"[;,]+\", str(raw))\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "\n",
    "def _make_prefixed(local_name: str, ns: str = \"ppeo\") -> str:\n",
    "    return f\"{ns}:{local_name}\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ontology loader & property registry\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class OntologyRegistry:\n",
    "    \"\"\"\n",
    "    Merges one or more OWL/TTL graphs and builds lookup tables for:\n",
    "      - class local-names  →  full URIs\n",
    "      - property local-names  →  (full_URI, domain_local, range_local, prop_type)\n",
    "      - column-mapping annotations from companion ontology\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *owl_paths: str | Path):\n",
    "        self.g = Graph()\n",
    "        for p in owl_paths:\n",
    "            fmt = \"turtle\" if str(p).endswith(\".ttl\") else None\n",
    "            self.g.parse(str(p), format=fmt)\n",
    "\n",
    "        self._classes: dict[str, URIRef] = {}       # local_name  → URI\n",
    "        self._props:   dict[str, dict]   = {}       # local_name  → info dict\n",
    "        self._col_map: dict[str, dict[str, str]] = defaultdict(dict)\n",
    "        # col_map[sheet_name][col_header] = prop_local_name\n",
    "\n",
    "        self._build_class_index()\n",
    "        self._build_property_index()\n",
    "        self._build_column_map()\n",
    "\n",
    "    # ------------------------------------------------------------------ index\n",
    "\n",
    "    def _build_class_index(self):\n",
    "        for cls in self.g.subjects(RDF.type, OWL.Class):\n",
    "            if isinstance(cls, URIRef):\n",
    "                ln = _local(cls)\n",
    "                self._classes[ln.lower()] = cls\n",
    "\n",
    "    def _build_property_index(self):\n",
    "        prop_types = {\n",
    "            OWL.DatatypeProperty: \"DataProperty\",\n",
    "            OWL.ObjectProperty:   \"ObjectProperty\",\n",
    "            OWL.AnnotationProperty: \"AnnotationProperty\",\n",
    "        }\n",
    "        for ptype, label in prop_types.items():\n",
    "            for prop in self.g.subjects(RDF.type, ptype):\n",
    "                if not isinstance(prop, URIRef):\n",
    "                    continue\n",
    "                ln = _local(prop)\n",
    "                domain_uri = self.g.value(prop, RDFS.domain)\n",
    "                range_uri  = self.g.value(prop, RDFS.range)\n",
    "                self._props[ln.lower()] = {\n",
    "                    \"uri\":    prop,\n",
    "                    \"local\":  ln,\n",
    "                    \"type\":   label,\n",
    "                    \"domain\": _local(domain_uri) if domain_uri and isinstance(domain_uri, URIRef) else None,\n",
    "                    \"range\":  _local(range_uri)  if range_uri  and isinstance(range_uri,  URIRef) else None,\n",
    "                }\n",
    "\n",
    "    def _build_column_map(self):\n",
    "        \"\"\"\n",
    "        Parse hydata:columnMappings annotations.\n",
    "        Format: \"Sheet^^ColumnHeader^^PropertyLocalName\"\n",
    "        \"\"\"\n",
    "        cm_pred = HYDATA.columnMappings\n",
    "        for _, _, obj in self.g.triples((None, cm_pred, None)):\n",
    "            parts = str(obj).split(\"^^\")\n",
    "            if len(parts) == 3:\n",
    "                sheet, col, prop = parts\n",
    "                self._col_map[sheet][col] = prop\n",
    "\n",
    "    # ------------------------------------------------------------------ query\n",
    "\n",
    "    def resolve_class(self, name: str) -> URIRef | None:\n",
    "        return self._classes.get(name.lower())\n",
    "\n",
    "    def resolve_property(self, name: str) -> dict | None:\n",
    "        return self._props.get(name.lower())\n",
    "\n",
    "    def column_to_property(self, sheet: str, col_header: str) -> str:\n",
    "        \"\"\"\n",
    "        Return the ontology local-name for a column header.\n",
    "        Priority: companion mapping → exact OWL match → camelCase OWL fuzzy → raw header.\n",
    "        \"\"\"\n",
    "        # 1. companion explicit mapping\n",
    "        if sheet in self._col_map and col_header in self._col_map[sheet]:\n",
    "            return self._col_map[sheet][col_header]\n",
    "\n",
    "        # 2. exact match in property index\n",
    "        if col_header.lower() in self._props:\n",
    "            return self._props[col_header.lower()][\"local\"]\n",
    "\n",
    "        # 3. strip \"Person:\" prefix\n",
    "        stripped = re.sub(r\"^[A-Za-z]+:\", \"\", col_header)\n",
    "        if stripped.lower() in self._props:\n",
    "            return self._props[stripped.lower()][\"local\"]\n",
    "\n",
    "        # 4. camelCase collapse (e.g., \"Public release date\" → \"publicReleaseDate\")\n",
    "        cc = re.sub(r\"\\s+(.)\", lambda m: m.group(1).upper(), col_header.strip()).replace(\" \", \"\")\n",
    "        cc = cc[0].lower() + cc[1:] if cc else cc\n",
    "        if cc.lower() in self._props:\n",
    "            return self._props[cc.lower()][\"local\"]\n",
    "\n",
    "        # 5. unknown → coin a hydata: term\n",
    "        safe = _slug(col_header)\n",
    "        return f\"hydata:{safe}\"\n",
    "\n",
    "    def property_info(self, local_name: str) -> dict:\n",
    "        \"\"\"Return property metadata dict, falling back to a default.\"\"\"\n",
    "        key = local_name.replace(\"hydata:\", \"\").lower()\n",
    "        return self._props.get(key, {\"local\": local_name, \"type\": \"DataProperty\",\n",
    "                                      \"domain\": None, \"range\": None})\n",
    "\n",
    "    def class_uri(self, sheet: str) -> str:\n",
    "        \"\"\"Return a curie for the class corresponding to a sheet.\"\"\"\n",
    "        uri = self.resolve_class(sheet)\n",
    "        if uri:\n",
    "            ln = _local(uri)\n",
    "            if str(uri).startswith(str(PPEO)):\n",
    "                return f\"ppeo:{ln}\"\n",
    "            if str(uri).startswith(str(HYDATA)):\n",
    "                return f\"hydata:{ln}\"\n",
    "        return f\"hydata:{sheet}\"\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Excel reader\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "class ExcelReader:\n",
    "    \"\"\"\n",
    "    Reads an xlsx workbook and returns sheets as lists of {col: value} dicts.\n",
    "    Handles the two sheet layouts:\n",
    "      - transposed  (investigation-like): row1=headers, row2=values\n",
    "      - record-per-row: row1=headers, row2..N=data rows\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str | Path):\n",
    "        self.wb = openpyxl.load_workbook(str(path))\n",
    "\n",
    "    @property\n",
    "    def sheet_names(self) -> list[str]:\n",
    "        return self.wb.sheetnames\n",
    "\n",
    "    def read_sheet(self, name: str) -> tuple[list[dict], bool]:\n",
    "        \"\"\"\n",
    "        Returns (rows, is_transposed).\n",
    "        Each row is a dict {header: value} with None keys stripped.\n",
    "        \"\"\"\n",
    "        ws = self.wb[name]\n",
    "        all_rows = list(ws.iter_rows(values_only=True))\n",
    "        if not all_rows:\n",
    "            return [], False\n",
    "\n",
    "        headers = [h for h in all_rows[0]]\n",
    "        data_rows = all_rows[1:]\n",
    "\n",
    "        # Detect transposed format: first header cell is \"property\" (case-insensitive)\n",
    "        is_transposed = (\n",
    "            headers and headers[0] is not None\n",
    "            and str(headers[0]).strip().lower() == \"property\"\n",
    "            and data_rows\n",
    "            and data_rows[0] and str(data_rows[0][0]).strip().lower() == \"value\"\n",
    "        )\n",
    "\n",
    "        result = []\n",
    "        if is_transposed:\n",
    "            # Single logical row: zip headers → values (skip index columns)\n",
    "            row_dict = {}\n",
    "            for h, v in zip(headers[1:], data_rows[0][1:]):\n",
    "                if h is not None and v is not None:\n",
    "                    row_dict[str(h).strip()] = v\n",
    "            if row_dict:\n",
    "                result.append(row_dict)\n",
    "        else:\n",
    "            for row in data_rows:\n",
    "                row_dict = {}\n",
    "                for h, v in zip(headers, row):\n",
    "                    if h is not None and v is not None:\n",
    "                        row_dict[str(h).strip()] = v\n",
    "                if row_dict:\n",
    "                    result.append(row_dict)\n",
    "\n",
    "        return result, is_transposed\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# JSON-LD builder\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Maps sheet names to ID column names\n",
    "ID_COLUMNS: dict[str, str] = {\n",
    "    \"investigation\": \"Investigation unique ID\",\n",
    "    \"study\":         \"hasID\",\n",
    "    \"person\":        \"Person:hasORCID\",   # fallback: generated UUID\n",
    "}\n",
    "\n",
    "# Date-like property names (will be converted to ISO date strings)\n",
    "DATE_PROPS = {\"hassubmissiondate\", \"publicreleasedate\", \"hasstartingdate\",\n",
    "              \"hasendingdate\", \"hasstartdatetime\", \"hasenddatetime\"}\n",
    "\n",
    "\n",
    "def _node_id(sheet: str, row: dict, idx: int) -> str:\n",
    "    \"\"\"\n",
    "    Derive a stable node @id from the row.\n",
    "    Uses the known ID column for the sheet, or generates a UUID.\n",
    "    \"\"\"\n",
    "    id_col = ID_COLUMNS.get(sheet.lower())\n",
    "    if id_col and id_col in row:\n",
    "        return str(row[id_col]).strip()\n",
    "    # fallback: try any column containing \"ID\" or \"id\" in its name\n",
    "    for k, v in row.items():\n",
    "        if \"id\" in k.lower() and v:\n",
    "            return str(v).strip()\n",
    "    return f\"urn:uuid:{uuid.uuid4()}\"\n",
    "\n",
    "\n",
    "def _value_for_jsonld(prop_local: str, raw_val: Any) -> Any:\n",
    "    \"\"\"\n",
    "    Coerce a raw cell value to an appropriate JSON-LD value representation.\n",
    "    Date cells come from openpyxl as datetime objects or as Excel serial ints.\n",
    "    \"\"\"\n",
    "    if raw_val is None:\n",
    "        return None\n",
    "\n",
    "    key = prop_local.lower().replace(\"hydata:\", \"\")\n",
    "\n",
    "    if key in DATE_PROPS or isinstance(raw_val, (datetime, date)):\n",
    "        ds = _parse_date(raw_val)\n",
    "        if ds:\n",
    "            return {\"@type\": \"xsd:date\", \"@value\": ds}\n",
    "\n",
    "    if isinstance(raw_val, float) and raw_val == int(raw_val):\n",
    "        # Could be a year or serial date\n",
    "        if key in DATE_PROPS:\n",
    "            ds = _parse_date(int(raw_val))\n",
    "            if ds:\n",
    "                return {\"@type\": \"xsd:date\", \"@value\": ds}\n",
    "        return int(raw_val)\n",
    "\n",
    "    if isinstance(raw_val, (int, float, bool)):\n",
    "        return raw_val\n",
    "\n",
    "    return str(raw_val).strip()\n",
    "\n",
    "\n",
    "def build_jsonld(\n",
    "    excel: ExcelReader,\n",
    "    registry: OntologyRegistry,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Main builder: iterates all sheets, validates classes, builds the @graph.\n",
    "    \"\"\"\n",
    "    context = dict(BASE_PREFIXES)\n",
    "    graph: list[dict] = []\n",
    "\n",
    "    # We need a two-pass approach so that object links (hasPart / partOf)\n",
    "    # can reference nodes defined in other sheets.\n",
    "    # Pass 1: build raw node dicts keyed by node @id\n",
    "    # Pass 2: resolve object-link properties into {\"@id\": …} references\n",
    "\n",
    "    # Raw nodes:  sheet_name → { node_id → node_dict }\n",
    "    all_nodes: dict[str, dict[str, dict]] = {}\n",
    "\n",
    "    for sheet_name in excel.sheet_names:\n",
    "        rows, is_transposed = excel.read_sheet(sheet_name)\n",
    "        if not rows:\n",
    "            print(f\"  [SKIP] sheet '{sheet_name}' is empty.\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "        # Validate class\n",
    "        class_uri = registry.resolve_class(sheet_name)\n",
    "        if class_uri is None:\n",
    "            print(f\"  [WARN] Class '{sheet_name}' not found in ontology; \"\n",
    "                  f\"using hydata:{sheet_name} as fallback.\", file=sys.stderr)\n",
    "        class_curie = registry.class_uri(sheet_name)\n",
    "\n",
    "        sheet_nodes: dict[str, dict] = {}\n",
    "        for idx, row in enumerate(rows):\n",
    "            node_id = _node_id(sheet_name, row, idx)\n",
    "            node: dict[str, Any] = {\n",
    "                \"@id\":   node_id,\n",
    "                \"@type\": class_curie,\n",
    "            }\n",
    "\n",
    "            for col_header, raw_val in row.items():\n",
    "                prop_local = registry.column_to_property(sheet_name, col_header)\n",
    "                pinfo      = registry.property_info(prop_local)\n",
    "\n",
    "                # skip if value is empty\n",
    "                if raw_val is None or str(raw_val).strip() == \"\":\n",
    "                    continue\n",
    "\n",
    "                # Object-link property → store raw string; resolve in pass 2\n",
    "                if (prop_local in OBJECT_LINK_PROPS\n",
    "                        or (pinfo.get(\"type\") == \"ObjectProperty\"\n",
    "                            and prop_local not in {\"hasExperimentalDesign\"})):\n",
    "                    # store as raw string list for later resolution\n",
    "                    ids = _split_ids(str(raw_val))\n",
    "                    if ids:\n",
    "                        node[f\"_raw_link_{prop_local}\"] = ids\n",
    "                    continue\n",
    "\n",
    "                val = _value_for_jsonld(prop_local, raw_val)\n",
    "                if val is None:\n",
    "                    continue\n",
    "\n",
    "                # Use a prefixed property name\n",
    "                if \":\" not in prop_local:\n",
    "                    # decide namespace: ppeo or hydata\n",
    "                    if registry.resolve_property(prop_local):\n",
    "                        pkey = f\"ppeo:{prop_local}\"\n",
    "                    else:\n",
    "                        pkey = f\"hydata:{prop_local}\"\n",
    "                else:\n",
    "                    pkey = prop_local\n",
    "\n",
    "                node[pkey] = val\n",
    "\n",
    "            sheet_nodes[node_id] = node\n",
    "        all_nodes[sheet_name] = sheet_nodes\n",
    "\n",
    "    # ------------------------------------------------------------------ pass 2\n",
    "    # Resolve object links; build the final graph\n",
    "\n",
    "    def _resolve_links(node: dict, all_nodes: dict) -> dict:\n",
    "        resolved = {k: v for k, v in node.items() if not k.startswith(\"_raw_link_\")}\n",
    "        for k, v in node.items():\n",
    "            if k.startswith(\"_raw_link_\"):\n",
    "                prop = k[len(\"_raw_link_\"):]\n",
    "                if \":\" not in prop:\n",
    "                    pkey = f\"ppeo:{prop}\" if registry.resolve_property(prop) else f\"hydata:{prop}\"\n",
    "                else:\n",
    "                    pkey = prop\n",
    "                refs = [{\"@id\": rid} for rid in v]\n",
    "                resolved[pkey] = refs if len(refs) > 1 else refs[0]\n",
    "        return resolved\n",
    "\n",
    "    # Build flat list, with Investigation's @graph embedding Studies\n",
    "    inv_nodes = all_nodes.get(\"Investigation\", {})\n",
    "    study_nodes = all_nodes.get(\"Study\", {})\n",
    "    person_nodes = all_nodes.get(\"Person\", {})\n",
    "\n",
    "    # Build study node list (resolved)\n",
    "    study_list = [_resolve_links(n, all_nodes) for n in study_nodes.values()]\n",
    "\n",
    "    # Build person node list → also contains partOf links to studies\n",
    "    person_list = [_resolve_links(n, all_nodes) for n in person_nodes.values()]\n",
    "\n",
    "    # Build investigation node(s) and embed studies via ppeo:hasPart\n",
    "    for inv_id, inv_node in inv_nodes.items():\n",
    "        inv_resolved = _resolve_links(inv_node, all_nodes)\n",
    "        # Ensure hasPart points to study list\n",
    "        if study_list:\n",
    "            inv_resolved[\"ppeo:hasPart\"] = [{\"@id\": s[\"@id\"]} for s in study_list]\n",
    "        graph.append(inv_resolved)\n",
    "\n",
    "    graph.extend(study_list)\n",
    "    graph.extend(person_list)\n",
    "\n",
    "    return {\n",
    "        \"@context\":  context,\n",
    "        \"@graph\":    graph,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cypher generator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def _cypher_label(curie: str) -> str:\n",
    "    \"\"\"Convert 'ppeo:study' → 'study' as a Neo4j label.\"\"\"\n",
    "    return curie.split(\":\")[-1].capitalize()\n",
    "\n",
    "\n",
    "def _cypher_value(val: Any) -> str:\n",
    "    \"\"\"Render a JSON-LD value as a Cypher literal.\"\"\"\n",
    "    if isinstance(val, dict):\n",
    "        # {\"@type\": \"xsd:date\", \"@value\": \"2024-07-05\"}  or  {\"@id\": \"...\"}\n",
    "        v = val.get(\"@value\", val.get(\"@id\", \"\"))\n",
    "        return f'\"{_cypher_escape(str(v))}\"'\n",
    "    if isinstance(val, bool):\n",
    "        return \"true\" if val else \"false\"\n",
    "    if isinstance(val, (int, float)):\n",
    "        return str(val)\n",
    "    if isinstance(val, str):\n",
    "        return f'\"{_cypher_escape(val)}\"'\n",
    "    return f'\"{_cypher_escape(str(val))}\"'\n",
    "\n",
    "\n",
    "def _cypher_escape(s: str) -> str:\n",
    "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace('\"', '\\\\\"').replace(\"\\n\", \"\\\\n\")\n",
    "\n",
    "\n",
    "def _prop_name(pkey: str) -> str:\n",
    "    \"\"\"'ppeo:hasName' → 'hasName'  (drop namespace prefix).\"\"\"\n",
    "    return pkey.split(\":\")[-1]\n",
    "\n",
    "\n",
    "RELATIONSHIP_PROPS = {\"hasPart\", \"partOf\", \"derivesFrom\", \"hasAffiliation\",\n",
    "                      \"hasLocation\", \"hasCountry\", \"hasContactInstitution\"}\n",
    "\n",
    "\n",
    "def build_cypher(jsonld: dict) -> str:\n",
    "    lines: list[str] = [\n",
    "        \"// ============================================================\",\n",
    "        \"// AUTO-GENERATED Cypher import script\",\n",
    "        \"// Target: Neo4j  |  Source: excel_to_jsonld.py\",\n",
    "        \"// ============================================================\",\n",
    "        \"\",\n",
    "        \"// Constraints (run once)\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (n:Investigation) REQUIRE n.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (n:Study)         REQUIRE n.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (n:Person)        REQUIRE n.id IS UNIQUE;\",\n",
    "        \"CREATE CONSTRAINT IF NOT EXISTS FOR (n:Institution)   REQUIRE n.name IS UNIQUE;\",\n",
    "        \"\",\n",
    "        \"// ---- Nodes ----\",\n",
    "        \"\",\n",
    "    ]\n",
    "\n",
    "    rel_queue: list[tuple[str, str, str, str]] = []\n",
    "    # (from_id, to_id, rel_type, from_label)\n",
    "\n",
    "    for node in jsonld.get(\"@graph\", []):\n",
    "        node_id  = node.get(\"@id\", \"\")\n",
    "        node_type = _cypher_label(node.get(\"@type\", \"Entity\"))\n",
    "        props: dict[str, Any] = {}\n",
    "\n",
    "        for k, v in node.items():\n",
    "            if k in (\"@id\", \"@type\"):\n",
    "                continue\n",
    "            pname = _prop_name(k)\n",
    "\n",
    "            # Object-link properties → queue as relationship\n",
    "            if pname in RELATIONSHIP_PROPS:\n",
    "                targets = v if isinstance(v, list) else [v]\n",
    "                for t in targets:\n",
    "                    if isinstance(t, dict) and \"@id\" in t:\n",
    "                        rel_queue.append((node_id, t[\"@id\"], pname.upper(), node_type))\n",
    "                continue\n",
    "\n",
    "            # Multi-value lists that are NOT object links → serialize as string\n",
    "            if isinstance(v, list):\n",
    "                props[pname] = json.dumps([\n",
    "                    vv.get(\"@value\", vv.get(\"@id\", str(vv))) if isinstance(vv, dict) else vv\n",
    "                    for vv in v\n",
    "                ])\n",
    "                continue\n",
    "\n",
    "            if isinstance(v, dict):\n",
    "                if \"@id\" in v:\n",
    "                    rel_queue.append((node_id, v[\"@id\"], pname.upper(), node_type))\n",
    "                    continue\n",
    "                if \"@value\" in v:\n",
    "                    props[pname] = v[\"@value\"]\n",
    "                continue\n",
    "\n",
    "            props[pname] = v\n",
    "\n",
    "        # Primary key defaults\n",
    "        props.setdefault(\"id\", node_id)\n",
    "\n",
    "        prop_str = \", \".join(\n",
    "            f\"{p}: {_cypher_value(val)}\" for p, val in props.items()\n",
    "        )\n",
    "        lines.append(f\"MERGE (n:{node_type} {{id: {_cypher_value(node_id)}}}) SET n += {{{prop_str}}};\")\n",
    "\n",
    "    lines += [\"\", \"// ---- Relationships ----\", \"\"]\n",
    "\n",
    "    # Helper to pick the right lookup key per label\n",
    "    def _lookup_key(label: str) -> str:\n",
    "        return \"id\"  # all our nodes use id as key\n",
    "\n",
    "    for from_id, to_id, rel_type, from_label in rel_queue:\n",
    "        # Try to determine the target label from the node type stored in graph\n",
    "        to_label = \"Entity\"\n",
    "        for node in jsonld.get(\"@graph\", []):\n",
    "            if node.get(\"@id\") == to_id:\n",
    "                to_label = _cypher_label(node.get(\"@type\", \"Entity\"))\n",
    "                break\n",
    "        # For unresolved targets (e.g. institutions as literals) we create them\n",
    "        if to_label == \"Entity\":\n",
    "            # might be a Study or Institution id\n",
    "            to_label = \"Study\"  # most common case for hasPart / partOf\n",
    "\n",
    "        lines.append(\n",
    "            f'MATCH (a:{from_label} {{id: \"{_cypher_escape(from_id)}\"}}) '\n",
    "            f'MATCH (b:{to_label}  {{id: \"{_cypher_escape(to_id)}\" }}) '\n",
    "            f'MERGE (a)-[:{rel_type}]->(b);'\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CLI\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    p = argparse.ArgumentParser(\n",
    "        description=\"Convert an OWL-backed Excel checklist to JSON-LD + Cypher.\"\n",
    "    )\n",
    "    p.add_argument(\"excel_file\",  help=\"Input .xlsx file\")\n",
    "    p.add_argument(\"owl_file\",    help=\"Base OWL ontology (.owl / .ttl / .rdf)\")\n",
    "    p.add_argument(\n",
    "        \"--companion\", \"-c\",\n",
    "        default=None,\n",
    "        help=\"Optional companion .ttl ontology (column mappings, extra terms)\",\n",
    "    )\n",
    "    p.add_argument(\"--output-jsonld\", \"-j\", default=\"output.jsonld\",\n",
    "                   help=\"Output JSON-LD file (default: output.jsonld)\")\n",
    "    p.add_argument(\"--output-cypher\", \"-q\", default=\"output.cypher\",\n",
    "                   help=\"Output Cypher file  (default: output.cypher)\")\n",
    "    p.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    print(f\"[1/4] Loading ontology: {args.owl_file}\")\n",
    "    owl_paths = [args.owl_file]\n",
    "    if args.companion:\n",
    "        print(f\"      + companion:       {args.companion}\")\n",
    "        owl_paths.append(args.companion)\n",
    "\n",
    "    registry = OntologyRegistry(*owl_paths)\n",
    "    print(f\"      Classes: {len(registry._classes)}   Properties: {len(registry._props)}\")\n",
    "\n",
    "    print(f\"\\n[2/4] Reading Excel: {args.excel_file}\")\n",
    "    excel = ExcelReader(args.excel_file)\n",
    "    print(f\"      Sheets: {excel.sheet_names}\")\n",
    "\n",
    "    print(\"\\n[3/4] Validating sheets against ontology ...\")\n",
    "    for sheet in excel.sheet_names:\n",
    "        uri = registry.resolve_class(sheet)\n",
    "        if uri:\n",
    "            print(f\"      ✓  '{sheet}' → <{uri}>\")\n",
    "        else:\n",
    "            print(f\"      ⚠  '{sheet}' has no direct class in ontology \"\n",
    "                  \"(will use hydata: fallback)\")\n",
    "\n",
    "    print(\"\\n[4/4] Building JSON-LD ...\")\n",
    "    jsonld = build_jsonld(excel, registry)\n",
    "\n",
    "    node_count = len(jsonld[\"@graph\"])\n",
    "    print(f\"      Graph nodes: {node_count}\")\n",
    "\n",
    "    # --- write JSON-LD ---\n",
    "    out_jld = Path(args.output_jsonld)\n",
    "    out_jld.write_text(json.dumps(jsonld, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(f\"\\n✅  JSON-LD written → {out_jld}\")\n",
    "\n",
    "    # --- write Cypher ---\n",
    "    cypher = build_cypher(jsonld)\n",
    "    out_cyp = Path(args.output_cypher)\n",
    "    out_cyp.write_text(cypher, encoding=\"utf-8\")\n",
    "    print(f\"✅  Cypher   written → {out_cyp}\")\n",
    "\n",
    "    # --- quick stats ---\n",
    "    labels: dict[str, int] = defaultdict(int)\n",
    "    for node in jsonld[\"@graph\"]:\n",
    "        labels[node.get(\"@type\", \"??\")] += 1\n",
    "    print(\"\\n--- Node-type summary ---\")\n",
    "    for lbl, cnt in sorted(labels.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  {lbl:30s}  {cnt}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
