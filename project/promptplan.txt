My dear student,

Thank you for bringing this to me. I have read your project description carefully, and I want to begin by saying that you have clearly grasped the central tension in our field: we are generating phenotyping data at an unprecedented rate, yet our ability to make that data intelligible—both to ourselves and to the machines we increasingly rely upon—has not kept pace. Your diagnosis of the problem is sound, and your instinct to build upon existing standards rather than inventing new ones is, frankly, a sign of mature thinking. Too many PhD projects begin with the conviction that everything before them was a mistake. You are not making that error. However, there are several areas where your framing can be sharpened, where the intellectual stakes can be raised, and where your language must become more precise if this work is to resonate with the critical gatekeepers in our community: journal reviewers, funding panels, and the standards bodies themselves.

Let me begin with the fundamental premise of your work. You state that MIAPPE currently lacks explicit constructs for sensors, platforms, and computational workflows. This is true, but the way you have phrased it suggests a kind of passive omission—as if the developers of MIAPPE simply forgot to include these elements. That is not the case, and you should not imply that it is. MIAPPE was deliberately designed to be agnostic with respect to measurement technology precisely because its primary architects came from a tradition of controlled-environment and manual field phenotyping, where the instrument was often a human with a ruler or a calibrated camera in a fixed installation. The challenge we now face is not that MIAPPE is incomplete in a trivial sense, but rather that the boundary between the biological experiment and the sensing system has collapsed. In modern field phenotyping, the sensor platform does not simply observe the experiment; it constitutes the experiment. The flight path of the unmanned aerial vehicle, the calibration of the multispectral sensor, the georeferencing of the orthomosaic—these are not ancillary metadata to be appended after the fact. They are, in a very real sense, part of the method. Your project must frame itself not as a mere extension of a checklist, but as a necessary re-theorization of what constitutes experimental metadata in an era of automated, high-throughput, and remote sensing-enabled plant science.

Your decision to integrate the Semantic Sensor Network ontology and its SOSA extension is conceptually sound, but I must caution you against treating this as a straightforward merging of two existing models. The alignment of MIAPPE and SOSA is not merely a technical exercise in finding edges between graphs; it is an exercise in ontological commitment. MIAPPE is organized around the biological study, the observation unit, the variable, and the trait. SOSA is organized around the sensor, the observable property, the procedure, and the sample. These are not parallel hierarchies that can be neatly overlaid. They operate at different levels of granularity and, in some cases, with conflicting assumptions about what constitutes the primary object of description. Your data model will need to make explicit decisions about where the SOSA concepts attach to the MIAPPE structure. Does a sensor attach to an observation unit? To a study? To a platform? To a specific timepoint? Each of these choices carries implications for how queries will be structured and how interoperability will be achieved. You should not simply state that you will find the edges; you should articulate a clear design philosophy for where and why those edges are drawn.

Your distinction between a checklist, a metadata standard, and an ontology requires further refinement. In your project overview, you describe these concepts in a way that is broadly correct but lacks the precision expected of a scholar working at the intersection of data science and plant phenotyping. A checklist is a human-readable set of required fields. A metadata standard, in the sense that MIAPPE is a standard, is a community-agreed specification of what those fields should be and how they should be formatted. An ontology, such as the one you propose to build, is a formal, machine-interpretable specification of a shared conceptualization—it includes not only terms but also the logical relationships between them. Your project moves across all three of these levels, and you must be explicit about how they relate. The extended checklist you propose in Phase One is not the ontology itself; it is a human-readable manifestation of a subset of the ontology. The data model you describe in Phase Two is the ontology. The knowledge graph converter in Phase Four is the technology that instantiates that ontology over actual datasets. You must not conflate these layers, and your writing must maintain this distinction with rigor.

Now let me speak to the structure of your phases. Your current presentation reads as a sequence of technical tasks, which is appropriate for a work plan but insufficient for a project description that must convey intellectual contribution. You need to articulate, for each phase, what question you are answering and why that question matters to the broader community. Phase One is not merely about drafting an extension; it is about determining whether a checklist-based approach, even when extended, can adequately capture the complexity of sensor-derived phenotyping data, or whether the checklist paradigm itself reaches its limit when confronted with the combinatorial explosion of platform-sensor-configuration combinations. Phase Two is not merely about converting that checklist into an ontology; it is about whether a modular, extensible ontology can accommodate the heterogeneity of our field without becoming so abstract as to be useless or so specific as to be non-reusable. Phase Three is not merely about building a knowledge base; it is about understanding the barriers to adoption and the sociology of standards-making in a community of practice that is not uniformly data-literate. Phase Four is not merely about converting tabular metadata into graphs; it is about whether the graph representation yields new capabilities for query and integration that are not achievable with relational or document-based approaches. And Phase Five—the graph embedding phase—is the most intellectually ambitious and, in its current form, the least developed. You state that you will develop, train, and validate embedding models, but you do not say against what. What is your ground truth for dataset similarity? Who decides that two experiments are conceptually similar? The danger here is that you will train a model to reproduce some existing classification scheme, thereby merely automating human judgment rather than discovering genuinely novel patterns. You must think carefully about what it means to validate a similarity measure in a domain where the very notion of similarity is contested.

I am also concerned about your treatment of the FAIR principles. You invoke them repeatedly, and rightly so, but you tend to treat them as a monolithic good rather than as a set of design principles that involve trade-offs. Findability, accessibility, interoperability, and reusability are not always mutually reinforcing. A highly interoperable representation may be less accessible to a researcher who simply wants to download a CSV file and plot some points. A richly annotated knowledge graph may be more findable by machines but less readable by humans. Your project should acknowledge these tensions and explain how your design decisions navigate them. Moreover, the FAIR principles are aspirational; no dataset is perfectly FAIR. Your project should articulate what degree of FAIRness is sufficient for what purposes. Do you need full machine-actionability, or is human-readable metadata with controlled vocabularies adequate for most reuse scenarios? Your emphasis on not developing new tools but rather referring to what already exists is commendable, but it must be reconciled with the fact that the tool you are ultimately building—the ontology, the converter, the embedding pipeline—is, in fact, new. There is no contradiction here; you are building new tools from existing components. But you should frame it that way, rather than suggesting that you are merely repackaging what others have done.

Your discussion of the current state of data in the domain, while accurate in its diagnosis, suffers from what I would call the lachrymose tone that afflicts much data-centric writing in our field. You describe data that is rarely shared, hard to digest for machines, and prone to interpretation. This is true, but it is also true of virtually every scientific domain that has undergone rapid technological transformation. The question is not why our data is in this state—we know why, it is because the technology outpaced the infrastructure—but rather what specific interventions are most likely to shift the equilibrium. Your project proposes one such intervention. Frame it as a constructive contribution, not a lamentation.

Now, let me offer you some specific recommendations for revision. First, reorient your introductory paragraph around a concrete scenario that illustrates the problem you are solving. Describe a researcher who has collected drone-based multispectral imagery over a wheat trial, has extracted canopy cover and NDVI, and wishes to compare their results with a similar experiment conducted in a different environment using a different sensor. Walk the reader through why this comparison is currently difficult and how your proposed framework would make it easier. This is not merely a rhetorical device; it will force you to be specific about what, exactly, your framework enables that is not currently possible.

Second, engage more deeply with the existing literature on semantic interoperability in phenotyping. You mention MIAPPE and SOSA, but there is also the Breeding API, the Agrisemantics initiative, the work of the Research Data Alliance Wheat Data Interoperability Working Group, and numerous efforts to develop application ontologies for specific crops or traits. Your project does not exist in a vacuum. You must situate it within this landscape and explain how it differs from, complements, or improves upon these other efforts. Do not simply assert that your approach is novel; demonstrate that you have understood what others have done and identified a genuine gap.

Third, your treatment of the Knowledge Graph Converter phase is currently underspecified. A knowledge graph is not merely a set of RDF triples; it is a graph-structured knowledge base that integrates data from multiple sources and supports reasoning over that data. What reasoning capabilities do you envision? What queries will your knowledge graph support that a conventional database cannot? You should identify one or two concrete competency questions—queries that your system should be able to answer—and use them to drive the design of both your ontology and your conversion pipeline. For example: Find all experiments conducted in Mediterranean climates that used multispectral imaging to assess drought tolerance in durum wheat. Or: Retrieve all datasets containing hyperspectral data collected from unmanned aerial vehicles at an altitude below 50 meters. If your knowledge graph cannot answer such questions, its value over a well-structured relational database is debatable.

Fourth, and this is perhaps the most important, you need to develop a clear position on the relationship between your work and the practice of phenotyping. You are not building infrastructure for its own sake; you are building infrastructure to enable better science. How will your framework change what phenotyping researchers actually do? Will it reduce the burden of metadata documentation by providing better tools? Will it enable new forms of meta-analysis and synthesis? Will it facilitate the training of machine learning models on larger, more diverse datasets? Your project description should explicitly connect each technical component to a tangible benefit for the research community. The embedding phase, for instance, should not be framed as an interesting application of graph neural networks; it should be framed as a method for discovering experimental contexts that produce similar phenotypic responses, thereby generating hypotheses about genotype-by-environment interactions that would not be apparent from individual studies.

Finally, attend to your prose. Academic writing in this domain values precision over flourish, but that does not mean it must be dry or riddled with typographical errors. Your current draft contains several misspellings—"experiements," "developement," "recorgnized," "there struggle," "demping"—that undermine the authority of your work. These are easily corrected, but their presence suggests a lack of attention to detail that will not serve you well when you submit to journals or apply for funding. Read your work aloud. Ask a colleague to read it. Print it and mark it with a red pen. The ideas are too important to be diminished by carelessness in their expression.

You have chosen a difficult but important problem, and your instincts about how to approach it are largely correct. The task now is to elevate your project description from a competent technical plan to a compelling intellectual argument. Think not only about what you are building, but about why it matters, for whom, and at what cost. Think about what your work assumes, what it enables, and what it forecloses. Think about the community you hope to serve and what will persuade them to adopt your framework rather than ignoring it or building their own. This is the difference between a student who executes a project and a scholar who shapes a field. You are capable of the latter. Now demonstrate it.

Come and see me next week. I want you to bring a revised draft that addresses these concerns, and I want you to be prepared to defend, not merely describe, your design decisions. We will go through it paragraph by paragraph. This is how rigorous science is made.



## Plan Rephrasing

- READ: https://dev.to/tim_derzhavets/building-your-first-knowledge-graph-a-practical-guide-from-schema-to-query-4eag

- Illustrate an example -> Take a set of examples from the Benchmark -> I would like to have the Datasets from Jim and From Julie (optional the one from Oli)

- Get an extensive overview within the literature of existing solutions or proposals for semantic interoperability in  